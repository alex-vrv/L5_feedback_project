{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UCr65EqgS-IV"
   },
   "source": [
    "# L5 to L2/3 feedback model - v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Iiy71SZS-IW"
   },
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ujH5WtqJS-IX"
   },
   "source": [
    "As in model v1, the network consists of the following populations:\n",
    "\n",
    "Excitatory:\n",
    "- L2/3 pyramidal neurons\n",
    "- L4 pyramidal neurons\n",
    "- L5a pyramidal neurons\n",
    "- L5b pyramidal neurons\n",
    "\n",
    "Inhibitory:\n",
    "- PV (in L2/3) interneurons\n",
    "\n",
    "With the following connections:\n",
    "- L5a -> L2/3\n",
    "- L5b -> PV\n",
    "- PV  -> L2/3\n",
    "- L4  -> L2/3\n",
    "\n",
    "\n",
    "L5a/b and L4 receive a (exp_rise,exp_decay) (albeit noisy),\n",
    "excitatory input current.\n",
    "\n",
    "The overall simulation consists of 3 parts*:\n",
    "1. Adjustment of inhibitory weights to achieve E/I balance ('balancing').\n",
    "2. Simulation only with the L4 population turned on.\n",
    "3. Simulations with temporal offsets (synchronous or asynchronous)\n",
    "\n",
    "**Tried matching Vm experimental values:**\n",
    "1. Random sampling of weights from exponential dist. (except for L4->L2/3)\n",
    "2. made more excitable PV model\n",
    "\t- Cm: 100pF (instead of previous 200)\n",
    "\t- consequently changed tau_m : now 10ms\n",
    "3. change in pyramidal cells model (L2/3,L4,L5a,L5b)\n",
    "    - Cm: 300pF (instead of previous 200)\n",
    "\t- consequently changed tau_m : now 30ms\n",
    "4. Changed baseline PV-->L2/3 weights (pre-balancing initialisation)\n",
    "\t- NOW: 2 nS \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3eeUy_IXS-IX"
   },
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1251,
     "status": "error",
     "timestamp": 1600534685383,
     "user": {
      "displayName": "Alex Vourvoukelis",
      "photoUrl": "",
      "userId": "07787819827310153908"
     },
     "user_tz": -180
    },
    "id": "Y-tWYNDTS-IY",
    "outputId": "98691b14-f4d2-438f-e86d-e64aea47bc41",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import brian2 as b2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from helpers import external_signals as ext_sign\n",
    "from scipy.signal import find_peaks,peak_widths,square\n",
    "import pandas as pd \n",
    "from scipy.stats import sem\n",
    "from matplotlib import cm\n",
    "from cycler import cycler\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['axes.spines.top'] = False\n",
    "mpl.rcParams['axes.spines.right'] = False\n",
    "\n",
    "colormap = plt.cm.plasma\n",
    "\n",
    "def cumulative_histogram(x, orientation = 'ascending'):\n",
    "    x = x.ravel()\n",
    "    x_sorted = np.sort(x)\n",
    "    if orientation == 'descending':\n",
    "        x_sorted = np.reverse(x_sorted)\n",
    "    percentile = np.arange(len(x))/np.float(len(x))\n",
    "    return(x_sorted, percentile)\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "def set_background(color):\n",
    "    script = (\"var cell = this.closest('.code_cell');\"\n",
    "              \"var editor = cell.querySelector('.input_area');\"\n",
    "              \"editor.style.background='{}';\"\n",
    "              \"this.parentNode.removeChild(this)\").format(color)\n",
    "    display(HTML('<img src onerror=\"{}\">'.format(script)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8aRe0284S-Ia"
   },
   "source": [
    "### Saving directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hEUWgiFQS-Ib",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#folder label indicating simulation details\n",
    "#notation: modeltype_stimulustype_activepopulationsorder\n",
    "simulation_label = \"discrete_synchronous_temporaloffset_10ms_200mswindow_L5double_w\"\n",
    "# if not os.path.exists(\"simulations/\"+simulation_label):\n",
    "#     os.mkdir(\"simulations/\"+simulation_label)\n",
    "\n",
    "locked_offset = 50\n",
    "#plot saving flag\n",
    "save_plots = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4sTrp6mpS-Id"
   },
   "source": [
    "### Functions for output measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ct6MoWdFS-Id",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Output measures\n",
    "def calc_total_spikes(population_label, balancing_time):\n",
    "    times = spike_monitor[population_label].t / b2.ms\n",
    "    mask = times >= balancing_time\n",
    "    total_spikes = np.sum(mask)\n",
    "    return (total_spikes)\n",
    "\n",
    "\n",
    "def calc_rate(population_label, balancing_time, total_time):\n",
    "    total_seconds = (total_time - balancing_time) / 1000\n",
    "    total_spikes = calc_total_spikes(population_label, balancing_time)\n",
    "    rate = total_spikes / population_parameters[population_label]['count'] / total_seconds\n",
    "    return (rate)\n",
    "\n",
    "def calc_peak_rate(population_label, smoothing_window_width):\n",
    "    peak_rate = max(rate_monitor[population_label].smooth_rate(window = 'gaussian', width = smoothing_window_width*b2.ms)/b2.Hz)\n",
    "    return peak_rate\n",
    "\n",
    "#calculates mean first spike latency to max values of input\n",
    "def calc_first_spike_latency(population_label,max_I_timepoints):\n",
    "    spiking_neurons = np.unique(spike_monitor[population_label].i)\n",
    "    population_latency_values = []\n",
    "    population_jitter_values = []\n",
    "    for spiking_neuron in spiking_neurons:\n",
    "        first_spike_latencies = []\n",
    "        spike_times = spike_monitor[population_label].t[spike_monitor[population_label].i == spiking_neuron]/b2.second*1000 # in ms            first_spike_latencies = []\n",
    "        for max_I in max_I_timepoints[max_I_timepoints<max(spike_times)]:\n",
    "            spike_times_mask = spike_times[(spike_times>max_I) & (spike_times<max_I+100)]\n",
    "            if len(spike_times_mask)!=0:\n",
    "                first_spike = spike_times_mask[0]\n",
    "                first_spike_latencies.append(first_spike - max_I)\n",
    "        if first_spike_latencies:\n",
    "            population_latency_values.append(np.mean(first_spike_latencies))\n",
    "            if len(first_spike_latencies)>1:\n",
    "                population_jitter_values.append(np.std(first_spike_latencies))\n",
    "            \n",
    "    return np.around(np.mean(population_latency_values),2), np.around(np.mean(population_jitter_values),2)\n",
    "\n",
    "def plot_first_spike_latency(population_label,max_I_timepoints):\n",
    "    spiking_neurons = np.unique(spike_monitor[population_label].i)\n",
    "    population_latency_values = []\n",
    "    population_jitter_values = []\n",
    "    for spiking_neuron in spiking_neurons:\n",
    "        first_spike_latencies = []\n",
    "        spike_times = spike_monitor[population_label].t[spike_monitor[population_label].i == spiking_neuron]/b2.second*1000 # in ms            first_spike_latencies = []\n",
    "        for max_I in max_I_timepoints[max_I_timepoints<max(spike_times)]:\n",
    "            spike_times_mask = spike_times[(spike_times>max_I) & (spike_times<max_I+100)]\n",
    "            if len(spike_times_mask)!=0:\n",
    "                first_spike = spike_times_mask[0]\n",
    "                first_spike_latencies.append(first_spike - max_I)\n",
    "        if first_spike_latencies:\n",
    "            population_latency_values.append(np.mean(first_spike_latencies))\n",
    "            if len(first_spike_latencies)>1:\n",
    "                population_jitter_values.append(np.std(first_spike_latencies))\n",
    "    #PLOT\n",
    "    fig, axes = plt.subplots(1, 2, sharex=False, sharey=False, figsize=(10, 5))\n",
    "\n",
    "    for ax, values, label in zip(axes,[population_latency_values, population_jitter_values], [\"First spike latency (ms)\",\"First spike jitter (ms)\"]):\n",
    "        ax.scatter(np.random.uniform(1,1.3,len(values)),values, marker = \".\", alpha = 0.5)\n",
    "        ax.errorbar(1.5, np.mean(values), yerr=np.std(values),marker = \"d\", c=\"y\", markersize = 10)\n",
    "        ax.set_ylabel(label)\n",
    "        ax.tick_params( axis='x',          # changes apply to the x-axis\n",
    "                        which='both',      # both major and minor ticks are affected\n",
    "                        bottom=False,      # ticks along the bottom edge are off\n",
    "                        top=False,         # ticks along the top edge are off\n",
    "                        labelbottom=False) # labels along the bottom edge are off\n",
    "        ax.set_xlim([0.8,1.7])\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N9KweOnYS-If"
   },
   "source": [
    "### Define external input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "b2.start_scope()\n",
    "\n",
    "total_neurons = 1000\n",
    "balancing_time = 5000\n",
    "interval_length = 3000\n",
    "only_whisker_time = interval_length\n",
    "offset_duration = 10 #in ms\n",
    "temporal_window = 200 #overall window in ms \n",
    "\n",
    "# test_all = []\n",
    "\n",
    "# for i in np.arange(-50,60,10):\n",
    "    \n",
    "#     positive_diff = i+10\n",
    "#     if positive_diff<60:\n",
    "#         test_all.append(tuple([i,positive_diff]))\n",
    "#     negative_diff = i-10\n",
    "#     if negative_diff>-60:\n",
    "#         test_all.append(tuple([i,negative_diff]))\n",
    "\n",
    "epochs = []\n",
    "epochs.append('Only L4 input')\n",
    "# offsetL5a = []\n",
    "# offsetL5b = []\n",
    "for offset in np.arange(int(-temporal_window/2),int(temporal_window/2+offset_duration),int(offset_duration)):\n",
    "    epochs.append('L5a '+str(offset)+' L5b '+str(offset)+' ms')\n",
    "\n",
    "# for offset1,offset2 in test_all:\n",
    "#     epochs.append('L5a '+str(offset1)+' L5b '+str(offset2)+' ms')\n",
    "#     offsetL5a.append(offset1)\n",
    "#     offsetL5b.append(offset2)\n",
    "# epochs.append('L5a '+str(0)+' L5b '+str(0)+' ms')\n",
    "# offsetL5a.append(0)\n",
    "# offsetL5b.append(0)\n",
    "        \n",
    "        \n",
    "total_tests = len(epochs)-1\n",
    "testing_time = total_tests * interval_length\n",
    "total_time = balancing_time + only_whisker_time + testing_time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# rythmic external input\n",
    "duty_cycle_length     = 500\n",
    "total_duty_cycles     = int((total_time + duty_cycle_length)/duty_cycle_length)\n",
    "duty_cycle_amplitude  = 100\n",
    "tau_rise = -5\n",
    "tau_decay = -0.5\n",
    "step_duration = 200\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "l4_neurons_are_active_input = ext_sign.generate_step_exprisedecay_signal(total_time = total_time,\n",
    "                                                                         duty_cycle_length = duty_cycle_length,\n",
    "                                                                         duty_cycle_amplitude=duty_cycle_amplitude,\n",
    "                                                                         offset=0,\n",
    "                                                                         tau_rise = tau_rise,\n",
    "                                                                         tau_decay = tau_decay)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L5a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "l5a_neurons_are_active_input = ext_sign.generate_step_exprisedecay_signal(total_time = total_time-testing_time,\n",
    "                                                                         duty_cycle_length = duty_cycle_length,\n",
    "                                                                         duty_cycle_amplitude=duty_cycle_amplitude,\n",
    "                                                                         offset=0,\n",
    "                                                                         tau_rise = tau_rise,\n",
    "                                                                         tau_decay = tau_decay)\n",
    "\n",
    "for counter,offset in enumerate(np.arange(int(-temporal_window/2),int(temporal_window/2+offset_duration),int(offset_duration))):\n",
    "# for counter,offset in enumerate(np.repeat(locked_offset,len(epochs)-1)):\n",
    "\n",
    "#     if counter < len(epochs): \n",
    "    l5a_neurons_are_active_input_temp = ext_sign.generate_step_exprisedecay_signal(total_time = interval_length,\n",
    "                                                                                         duty_cycle_length = duty_cycle_length,\n",
    "                                                                                         duty_cycle_amplitude=duty_cycle_amplitude,\n",
    "                                                                                         offset=offset,\n",
    "                                                                                         tau_rise = tau_rise,\n",
    "                                                                                         tau_decay = tau_decay)\n",
    "#     else:\n",
    "#         l5a_neurons_are_active_input_temp = 0*ext_sign.generate_step_exprisedecay_signal(total_time = interval_length,\n",
    "#                                                                                              duty_cycle_length = duty_cycle_length,\n",
    "#                                                                                              duty_cycle_amplitude=duty_cycle_amplitude,\n",
    "#                                                                                              offset=offset,\n",
    "#                                                                                              tau_rise = tau_rise,\n",
    "#                                                                                              tau_decay = tau_decay)\n",
    "        \n",
    "    l5a_neurons_are_active_input = np.concatenate((l5a_neurons_are_active_input,l5a_neurons_are_active_input_temp))\n",
    "    \n",
    "\n",
    "# l5a_neurons_are_active_input = np.concatenate((l5a_neurons_are_active_input,np.zeros(only_whisker_time)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "    \n",
    "    \n",
    "l5b_neurons_are_active_input = ext_sign.generate_step_exprisedecay_signal(total_time = total_time-testing_time,\n",
    "                                                                         duty_cycle_length = duty_cycle_length,\n",
    "                                                                         duty_cycle_amplitude=duty_cycle_amplitude,\n",
    "                                                                         offset=0,\n",
    "                                                                         tau_rise = tau_rise,\n",
    "                                                                         tau_decay = tau_decay)\n",
    "\n",
    "\n",
    "\n",
    "for counter,offset in enumerate(np.arange(int(-temporal_window/2),int(temporal_window/2+offset_duration),int(offset_duration))):\n",
    "#     if counter < len(epochs): \n",
    "    l5b_neurons_are_active_input_temp = ext_sign.generate_step_exprisedecay_signal(total_time = interval_length,\n",
    "                                                                                         duty_cycle_length = duty_cycle_length,\n",
    "                                                                                         duty_cycle_amplitude=duty_cycle_amplitude,\n",
    "                                                                                         offset=offset,\n",
    "                                                                                         tau_rise = tau_rise,\n",
    "                                                                                         tau_decay = tau_decay)\n",
    "#     else:\n",
    "#         l5b_neurons_are_active_input_temp = 0*ext_sign.generate_step_exprisedecay_signal(total_time = interval_length,\n",
    "#                                                                                              duty_cycle_length = duty_cycle_length,\n",
    "#                                                                                              duty_cycle_amplitude=duty_cycle_amplitude,\n",
    "#                                                                                              offset=offset,\n",
    "#                                                                                              tau_rise = tau_rise,\n",
    "#                                                                                              tau_decay = tau_decay)\n",
    "        \n",
    "    l5b_neurons_are_active_input = np.concatenate((l5b_neurons_are_active_input,l5b_neurons_are_active_input_temp))\n",
    "    \n",
    "# l5b_neurons_are_active_input = np.concatenate((l5b_neurons_are_active_input,np.zeros(only_whisker_time)))\n",
    "                                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding noise & constructing TimedArrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# arr_rythmic = np.concatenate((balancing_time_input,only_whisker_input,l5a_neurons_are_active_input,l5b_neurons_are_active_imput))\n",
    "I_rythmic_L4 = b2.TimedArray(l4_neurons_are_active_input*b2.pA, dt=1*b2.ms)\n",
    "I_rythmic_L5a = b2.TimedArray(l5a_neurons_are_active_input*b2.pA, dt=1*b2.ms)\n",
    "I_rythmic_L5b = b2.TimedArray(l5b_neurons_are_active_input*b2.pA, dt=1*b2.ms)\n",
    "\n",
    "# # random background noise (time-varying)\n",
    "# noise_time_scale = 10.\n",
    "# # arr_noise = 50 + 50 * np.random.randn(int(total_time / noise_time_scale), total_neurons)\n",
    "# arr_noise = duty_cycle_amplitude + duty_cycle_amplitude * np.random.randn(int(total_time / noise_time_scale), total_neurons)\n",
    "# I_random  = b2.TimedArray(arr_noise*b2.pA, dt=noise_time_scale*b2.ms)\n",
    "# random background noise (time-varying)\n",
    "noise_time_scale = 10\n",
    "\n",
    "arr_noise = duty_cycle_amplitude/2 + duty_cycle_amplitude/2 * np.random.randn(int(total_time / noise_time_scale), total_neurons)\n",
    "\n",
    "# I_random_L4  = b2.TimedArray(np.ones(total_time)*(duty_cycle_amplitude * np.random.randn(int(total_time / noise_time_scale), total_neurons))*b2.pA, dt=noise_time_scale*b2.ms)\n",
    "I_random_L4  = b2.TimedArray(arr_noise*b2.pA, dt=noise_time_scale*b2.ms)\n",
    "I_random_L5a = b2.TimedArray(arr_noise*b2.pA, dt=noise_time_scale*b2.ms)\n",
    "I_random_L5b = b2.TimedArray(arr_noise*b2.pA, dt=noise_time_scale*b2.ms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect external input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iIcuek-3S-Ii",
    "outputId": "08843417-3a67-4d2b-c128-18b770300cef",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### General#### General#### General#### L4#### L5a#### L5b### Inspect external inputfig = plt.figure(figsize = (15,5))\n",
    "fig = plt.figure(figsize = (15,5))\n",
    "plt.plot((I_rythmic_L5a.values*(10**(12)))[0:1000])\n",
    "plt.ylabel(\"Amplitude (pA)\")\n",
    "stim_f = total_duty_cycles/total_time*1000\n",
    "plt.title(\"Stimulus frequency: %.2f Hz\" % stim_f)\n",
    "fig.axes[0].text(0.8,-0.1,simulation_label,\n",
    "        horizontalalignment='left',\n",
    "        verticalalignment='top',\n",
    "        transform=fig.axes[0].transAxes)\n",
    "max_I_timepoints = find_peaks(I_rythmic_L4.values)[0]\n",
    "\n",
    "# if save_plots == True:\n",
    "# fig.savefig('simulations/'+simulation_label+'/stimulus.pdf')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SVGmUwXTS-Il",
    "outputId": "7f1a5278-85b2-4299-c910-c01ddb2fe64c",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# max_I_timepoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AR4Z3Yu6S-In"
   },
   "source": [
    "### Define network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bnw7rmDrS-Io",
    "outputId": "0cfba699-c45a-4afa-e41c-26c312415d17",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "set_background('lightblue')\n",
    "\n",
    "# Neuron groups\n",
    "population_parameters = dict()\n",
    "population_parameters['L4']   = {'name' :'L4',   'count':int(total_neurons / 5), 'color':'salmon'}\n",
    "population_parameters['L2/3'] = {'name' :'L2/3', 'count':int(total_neurons / 5), 'color':'red'}\n",
    "population_parameters['PV']   = {'name' :'PV',   'count':int(total_neurons / 5), 'color':'cornflowerblue'}\n",
    "population_parameters['L5a']  = {'name' :'L5a',  'count':int(total_neurons / 5), 'color': colormap.colors[0]}\n",
    "population_parameters['L5b']  = {'name' :'L5b',  'count':int(total_neurons / 5), 'color': colormap.colors[-1]}\n",
    "\n",
    "# Neuron properties\n",
    "E_L   = -60.   *b2.mV       # Resting potential\n",
    "E_R   = -80.   *b2.mV       # Inhibitory reversal potential\n",
    "V_th  = -50.   *b2.mV       # Spiking threshold\n",
    "t_ref =   5.   *b2.ms       # Refractory period\n",
    "gl    =  10.   *b2.nsiemens # Leak conductance\n",
    "C_m   = 300.   *b2.pfarad   # Membrane capacitance\n",
    "C_m_PV   = 100. *b2.pfarad   # Membrane capacitance PV interneuron\n",
    "tau_m = C_m/gl              # Membrane time constant\n",
    "tau_m_PV = C_m_PV/gl        # Membrane time constant PV interneuron\n",
    "gmax  = 100.   *b2.nsiemens # Maximum inhibitory weight\n",
    "I_ext =   0.   *b2.pA       # Constant external input current (where applicable)\n",
    "\n",
    "base_model = '''\n",
    "dv/dt    = -(v - E_L)/tau_m - I/C_m + I_ext/C_m : volt (unless refractory)\n",
    "dg_ex/dt = -g_ex/tau_ex_decay                   : siemens\n",
    "dg_in/dt = -g_in/tau_in_decay                   : siemens\n",
    "I_ex     =  g_ex*v                              : amp\n",
    "I_in     =  g_in*(v - E_R)                      : amp\n",
    "I        =  I_ex + I_in                         : amp\n",
    "dA/dt    =  -A / tau_m                          : siemens\n",
    "'''\n",
    "\n",
    "base_model_PV = '''\n",
    "dv/dt    = -(v - E_L)/tau_m_PV - I/C_m_PV + I_ext/C_m_PV : volt (unless refractory)\n",
    "dg_ex/dt = -g_ex/tau_ex_decay                   : siemens\n",
    "dg_in/dt = -g_in/tau_in_decay                   : siemens\n",
    "I_ex     =  g_ex*v                              : amp\n",
    "I_in     =  g_in*(v - E_R)                      : amp\n",
    "I        =  I_ex + I_in                         : amp\n",
    "dA/dt    =  -A / tau_m_PV                       : siemens\n",
    "'''\n",
    "\n",
    "base_model_L23 = '''\n",
    "dv/dt    = -(v - E_L)/tau_m + I_syn/tau_m - I/C_m + I_ext/C_m : volt (unless refractory)\n",
    "dg_ex/dt = -g_ex/tau_ex_decay                   : siemens\n",
    "dg_in/dt = -g_in/tau_in_decay                   : siemens\n",
    "I_ex     =  g_ex*v                              : amp\n",
    "I_in     =  g_in*(v - E_R)                      : amp\n",
    "I        =  I_ex + I_in                         : amp\n",
    "dA/dt    =  -A / tau_m                          : siemens\n",
    "I_syn                                           : volt\n",
    "'''\n",
    "\n",
    "reset = '''\n",
    "v = E_L\n",
    "A += 1 * nsiemens\n",
    "'''\n",
    "\n",
    "externally_driven = base_model + '''\n",
    "I_ext    =  I_rythmic(t) + I_random(t, i)       : amp\n",
    "'''\n",
    "\n",
    "net = b2.Network(b2.collect())\n",
    "\n",
    "populations = dict()\n",
    "for label in ['L4', 'L5a', 'L5b']:\n",
    "    populations[label] = b2.NeuronGroup(population_parameters[label]['count'],\n",
    "                                        model      = base_model +f'''\n",
    "                                        I_ext    =  I_rythmic_{label}(t) + I_random_{label}(t, i)       : amp\n",
    "                                        ''',\n",
    "                                        threshold  = 'v > V_th',\n",
    "                                        reset      = reset,\n",
    "                                        refractory = t_ref,\n",
    "                                        method     = 'euler'\n",
    "        )\n",
    "for label in ['L2/3']:\n",
    "    populations[label] = b2.NeuronGroup(population_parameters[label]['count'],\n",
    "                                        model = base_model_L23,\n",
    "                                        threshold  = 'v > V_th',\n",
    "                                        reset      = reset,\n",
    "                                        refractory = t_ref,\n",
    "                                        method     = 'euler'\n",
    "    )\n",
    "\n",
    "\n",
    "for label in ['PV']:\n",
    "    populations[label] = b2.NeuronGroup(population_parameters[label]['count'],\n",
    "                                        model      = base_model_PV,\n",
    "                                        threshold  = 'v > V_th',\n",
    "                                        reset      = reset,\n",
    "                                        refractory = t_ref,\n",
    "                                        method     = 'euler'\n",
    "    )\n",
    "# randomize initial membrane potentials\n",
    "for label, population in populations.items():\n",
    "    population.v = 'E_L + (V_th - E_L) * rand()'\n",
    "\n",
    "net.add(populations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z2r39i94S-Iq"
   },
   "source": [
    "### Define connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OK1ufjshS-Iq",
    "outputId": "6f0871e5-4143-4337-f7f2-7370ce3fc028",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "set_background('lightblue')\n",
    "\n",
    "\n",
    "# Synapse parameters\n",
    "# w_exc        =   1.  *b2.nsiemens  # Excitatory weight\n",
    "# w_inh        =   1.  *b2.nsiemens  # Inhibitory weight\n",
    "tau_ex_decay =   5.  *b2.ms        # Glutamatergic synaptic time constant\n",
    "tau_in_decay =   10.  *b2.ms        # GABAergic synaptic time constant\n",
    "eta          =   0.1               # Learning rate\n",
    "tau_stdp     =  20.  *b2.ms        # STDP time constant\n",
    "rho          =  10.  *b2.Hz        # Target excitatory population rate\n",
    "beta         =  rho * tau_stdp * 2 # Target rate parameter\n",
    "\n",
    "# Connection probabilities for L5 -> L2/3\n",
    "p = dict()\n",
    "p[('L5a', 'L2/3')] = 10\n",
    "# p[('L5b', 'L2/3')] = 10\n",
    "# p[('L5a', 'PV')  ] = 10\n",
    "p[('L5b', 'PV')  ] = 10\n",
    "p[('PV',  'L2/3')] = 10\n",
    "p[('L4',  'L2/3')] = 10\n",
    "\n",
    "w_mean = dict()\n",
    "w_mean[('L5a', 'L2/3')] = 4. * b2.nsiemens\n",
    "# w_mean[('L5b', 'L2/3')] = 2. * b2.nsiemens\n",
    "# w_mean[('L5a', 'PV')  ] = 2. * b2.nsiemens\n",
    "w_mean[('L5b', 'PV')  ] = 4. * b2.nsiemens\n",
    "w_mean[('PV',  'L2/3')] = 5. * b2.nsiemens\n",
    "w_mean[('L4',  'L2/3')] = 5. * b2.nsiemens\n",
    "\n",
    "# rescale\n",
    "p_scale = 1 / 100\n",
    "p = {k : p_scale * v for k, v in p.items()}\n",
    "\n",
    "w_scale = 0.5\n",
    "w_mean = {k : w_scale * v for k, v in w_mean.items()}\n",
    "\n",
    "connections = dict()\n",
    "for (source_label, target_label) in [('L5a', 'L2/3'), ('L5b', 'PV')]:\n",
    "    source = populations[source_label]\n",
    "    target = populations[target_label]\n",
    "    connections[(source_label, target_label)] = b2.Synapses(source, target,\n",
    "                                                            model='w : siemens',\n",
    "                                                            on_pre='g_ex_post += w',\n",
    "                                                            delay = 1. * b2.ms\n",
    "    )\n",
    "    connections[(source_label, target_label)].connect(p=p[(source_label, target_label)])\n",
    "    connections[(source_label, target_label)].w = np.random.exponential(w_mean[(source_label, target_label)],len(connections[(source_label, target_label)]))*b2.siemens\n",
    "\n",
    "for (source_label, target_label) in [('L4', 'L2/3')]:\n",
    "    source = populations[source_label]\n",
    "    target = populations[target_label]\n",
    "    connections[(source_label, target_label)] = b2.Synapses(\n",
    "        source, target,\n",
    "        model='w : siemens',\n",
    "        on_pre='g_ex_post += w',\n",
    "        delay = 1. * b2.ms\n",
    "        )\n",
    "#         '''\n",
    "#         w : siemens\n",
    "#         delta : siemens\n",
    "#         dApre/dt  = -Apre  / tau_stdp : siemens (event-driven)\n",
    "#         dApost/dt = -Apost / tau_stdp : siemens (event-driven)\n",
    "#         ''',\n",
    "#         on_pre='''\n",
    "#         Apre += 1.*nsiemens\n",
    "#         delta += Apost\n",
    "#         g_ex_post += w\n",
    "#         ''',\n",
    "#         on_post='''\n",
    "#         Apost += 1.*nsiemens\n",
    "#         delta -= Apre\n",
    "#         '''\n",
    "#     )\n",
    "connections[('L4', 'L2/3')].connect(p=p[('L4', 'L2/3')])\n",
    "connections[('L4', 'L2/3')].w = w_mean[(source_label, target_label)]\n",
    "# connections[('L4', 'L2/3')].delta = 0. * b2.siemens\n",
    "\n",
    "# Vogels-Sprekler synapse to achieve and E/I balance\n",
    "connections[('PV', 'L2/3')] = b2.Synapses(\n",
    "    populations['PV'], populations['L2/3'],\n",
    "    model= '''\n",
    "    dz1/dt = - z1 / I_tau_rise         : 1 (clock-driven)\n",
    "    dz2/dt = (- z2 + z1) / I_tau_decay : 1 (clock-driven)\n",
    "    I_syn_post = v_I_syn * z2          : volt (summed)\n",
    "    v_I_syn                            : volt\n",
    "\n",
    "    w : siemens\n",
    "    dApre/dt  = -Apre  / tau_stdp : siemens (event-driven)\n",
    "    dApost/dt = -Apost / tau_stdp : siemens (event-driven)\n",
    "    ''' ,\n",
    "    on_pre='''\n",
    "    Apre += 1.*nsiemens\n",
    "    w = clip(w + (Apost - beta*nS) * eta, 0, gmax)\n",
    "    g_in_post += w\n",
    "    z1 += 1\n",
    "    ''',\n",
    "    on_post='''\n",
    "    Apost += 1.*nsiemens\n",
    "    w = clip(w + Apre * eta, 0, gmax)\n",
    "    ''',\n",
    "    namespace = dict(\n",
    "    I_tau_rise = 3. * b2.msecond, # IPSP rise time\n",
    "    I_tau_decay = 40. * b2.msecond # IPSP decay time\n",
    "    ),\n",
    "    delay = 0.8 * b2.msecond,\n",
    "    method = 'euler'\n",
    ")\n",
    "\n",
    "connections[('PV', 'L2/3')].connect(p=p[('PV', 'L2/3')])\n",
    "connections[('PV', 'L2/3')].w = w_mean[(source_label, target_label)]\n",
    "\n",
    "net.add(connections)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nWMHf1zKS-Is"
   },
   "source": [
    "### Setup monitors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IK7pbkHtS-Is",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Setup monitors# Create spike, state and rate monitors\n",
    "spike_monitor = dict()\n",
    "state_monitor = dict()\n",
    "rate_monitor  = dict()\n",
    "for label in populations:\n",
    "    spike_monitor[label] = b2.SpikeMonitor(populations[label])\n",
    "    state_monitor[label] = b2.StateMonitor(populations[label], variables=True, record=True)\n",
    "    rate_monitor[label]  = b2.PopulationRateMonitor(populations[label])\n",
    "net.add(spike_monitor)\n",
    "net.add(state_monitor)\n",
    "net.add(rate_monitor)\n",
    "\n",
    "# connection_monitor = b2.StateMonitor(connections[('L4', 'L2/3')],\n",
    "#                                      variables='delta',\n",
    "#                                      dt=1.*b2.ms,\n",
    "#                                      record=True)\n",
    "# net.add(connection_monitor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uNRd-FhOS-Iu"
   },
   "source": [
    "### Run simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IOfjft4zS-Iv",
    "outputId": "9307943c-c462-440d-8228-b181dff5bd7f",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# balance the network using inhibitory plasticity\n",
    "net.run(balancing_time * b2.ms, report='stdout', report_period=10*b2.second)\n",
    "\n",
    "# turn off inhibitory plasticity\n",
    "# connections[('PV', 'L2/3')].namespace['eta'] = 0.\n",
    "eta = 0.0\n",
    "\n",
    "populations['L5a'].active = False\n",
    "populations['L5b'].active = False\n",
    "\n",
    "net.run(only_whisker_time * b2.ms, report='stdout', report_period=10*b2.second)\n",
    "\n",
    "# populations['L5a'].active = True\n",
    "populations['L5a'].active = True\n",
    "populations['L4'].active = True\n",
    "\n",
    "\n",
    "net.run(testing_time * b2.ms, report='stdout', report_period=10*b2.second)\n",
    "\n",
    "\n",
    "\n",
    "# # turn off L4 neurons\n",
    "# populations['L4'].active = False\n",
    "\n",
    "# # turn off all L5 neurons\n",
    "# populations['L5a'].active = False\n",
    "# populations['L5b'].active = False\n",
    "\n",
    "\n",
    "# net.run(l4_neurons_are_active * b2.ms, report='stdout', report_period=10*b2.second)\n",
    "\n",
    "# # turn back on L5a neurons\n",
    "# # turn off L5b neurons\n",
    "# populations['L5a'].active = True\n",
    "\n",
    "# net.run(l5a_neurons_are_active * b2.ms, report='stdout', report_period=10*b2.second)\n",
    "\n",
    "# # turn off L5a neurons\n",
    "# # turn back on L5ab neurons\n",
    "# populations['L5a'].active = False\n",
    "# populations['L5b'].active = True\n",
    "\n",
    "\n",
    "# net.run(l5b_neurons_are_active * b2.ms, report='stdout', report_period=10*b2.second)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C9OKrYq7S-Ix"
   },
   "source": [
    "### Output measures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Title builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "simulation_label_split = simulation_label.split('_')\n",
    "model_title = simulation_label_split[0].capitalize()\n",
    "if 'temporaloffset' in simulation_label_split:\n",
    "    if 'asynchronous' in simulation_label_split:\n",
    "        synchronicity_title = 'asynchronous'\n",
    "        if 'L5bvaried' in simulation_label_split:\n",
    "            L5a_label = [i for i in simulation_label_split if 'L5a' in i][-1]\n",
    "            L5a_title = 'L5a locked ('+L5a_label[3:]+'ms)'\n",
    "            L5b_title = 'L5b varied'\n",
    "        if 'L5avaried' in simulation_label_split:\n",
    "            L5a_title = 'L5a varied'\n",
    "            L5b_label = [i for i in simulation_label_split if 'L5b' in i][-1]\n",
    "            L5b_title = 'L5b locked ('+L5b_label[3:]+'ms)'\n",
    "    if 'synchronous' in simulation_label_split:\n",
    "        synchronicity_title = 'synchronous'\n",
    "\n",
    "general_simulation_title = model_title+' model ('+synchronicity_title+' temporal offset)'\n",
    "# if 'asynchronous' in simulation_label_split:\n",
    "#     general_simulation_subtitle = L5a_title+' - '+L5b_title\n",
    "#     general_simulation_subtitle = general_simulation_subtitle.rjust(int(0.75*len(general_simulation_title)), ' ')\n",
    "#     general_simulation_title = general_simulation_title+'\\n'+general_simulation_subtitle        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MnK-gJFbS-Iy",
    "outputId": "c0f301b5-ce56-403f-def3-284b2f0e99c2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set_background('lightblue')\n",
    "\n",
    "# print('Total spikes (per population):\\n')\n",
    "\n",
    "# for label in ['L4', 'L5a', 'L5b', 'L2/3', 'PV']:\n",
    "#     print(f\"{label} : {calc_total_spikes(label,balancing_time)} spikes\")### Output measures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cAfm8mnqS-I0",
    "outputId": "8bb83e46-1c0c-41d1-a7c3-e20d1ae545c9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set_background('lightblue')\n",
    "\n",
    "# print('Total firing rates (population average):\\n')\n",
    "\n",
    "# for label in ['L4', 'L5a', 'L5b', 'L2/3', 'PV']:\n",
    "#     print(f\"{label} : {np.around(calc_rate(label,balancing_time,total_time),2)} Hz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cJl5RAxeS-I1",
    "outputId": "ec7266b2-937c-4607-e559-a60934ab1504",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# set_background('lightblue')\n",
    "\n",
    "# smoothingWindowWidth = 25 # in ms\n",
    "# print('Peak spike rate (per population):\\n')\n",
    "\n",
    "# for label in ['L4', 'L5a', 'L5b', 'L2/3', 'PV']:\n",
    "#     print(f\"{label} : {np.around(calc_peak_rate(label,smoothing_window_width=25),2)} Hz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YoYnq7m1S-I3",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# set_background('lightblue')\n",
    "\n",
    "# print('L2/3 first spike latency (L5A): ', calc_first_spike_latency('L2/3',max_I_timepoints[max_I_timepoints>balancing_time])[0], 'ms')\n",
    "# print('L2/3 first spike jitter (all epochs): ', calc_first_spike_latency('L2/3',max_I_timepoints[max_I_timepoints>balancing_time])[1], 'ms')\n",
    "\n",
    "# print('L2/3 first spike latency (L5a/L5b co-active): ', calc_first_spike_latency('L2/3',max_I_timepoints[max_I_timepoints>3000])[0], 'ms')\n",
    "# print('L2/3 first spike jitter (L5a/L5b co-active): ', calc_first_spike_latency('L2/3',max_I_timepoints[max_I_timepoints>3000])[1], 'ms')\n",
    "\n",
    "# fig1 = plot_first_spike_latency('L2/3',max_I_timepoints[np.where(np.logical_and(max_I_timepoints>balancing_time, max_I_timepoints<2000))])\n",
    "# fig1.suptitle(epochs[0], fontsize=16)\n",
    "# fig1.axes[0].set_title('L2/3 first spike latency: '+str(calc_first_spike_latency('L2/3',max_I_timepoints[np.where(np.logical_and(max_I_timepoints>balancing_time, max_I_timepoints<2000))])[0])+ ' ms')\n",
    "# fig1.axes[1].set_title('L2/3 first spike jitter: '+str(calc_first_spike_latency('L2/3',max_I_timepoints[np.where(np.logical_and(max_I_timepoints>balancing_time, max_I_timepoints<2000))])[1])+ ' ms')\n",
    "# fig1.axes[-1].text(0.5,-0.1,simulation_label,\n",
    "#         horizontalalignment='left',\n",
    "#         verticalalignment='top',\n",
    "#         transform=fig1.axes[-1].transAxes)\n",
    "# if save_plots == True:\n",
    "#     fig1.savefig('simulations/'+simulation_label+'/'+epochs[0]+'_firstspike.pdf')\n",
    "\n",
    "\n",
    "# fig2 = plot_first_spike_latency('L2/3',max_I_timepoints[np.where(np.logical_and(max_I_timepoints>2000, max_I_timepoints<3000))])\n",
    "# fig2.suptitle(epochs[1], fontsize=16)\n",
    "# fig2.axes[0].set_title('L2/3 first spike latency: '+str(calc_first_spike_latency('L2/3',max_I_timepoints[np.where(np.logical_and(max_I_timepoints>2000, max_I_timepoints<3000))])[0])+ ' ms')\n",
    "# fig2.axes[1].set_title('L2/3 first spike jitter: '+str(calc_first_spike_latency('L2/3',max_I_timepoints[np.where(np.logical_and(max_I_timepoints>2000, max_I_timepoints<3000))])[1])+ ' ms')\n",
    "# fig2.axes[-1].text(0.5,-0.1,simulation_label,\n",
    "#         horizontalalignment='left',\n",
    "#         verticalalignment='top',\n",
    "#         transform=fig2.axes[-1].transAxes)\n",
    "# if save_plots == True:\n",
    "#     fig2.savefig('simulations/'+simulation_label+'/'+epochs[1]+'_firstspike.pdf')\n",
    "\n",
    "\n",
    "# fig3 = plot_first_spike_latency('L2/3',max_I_timepoints[np.where(np.logical_and(max_I_timepoints>3000, max_I_timepoints<4000))])\n",
    "# fig3.suptitle(epochs[2], fontsize=16)\n",
    "# fig3.axes[0].set_title('L2/3 first spike latency: '+str(calc_first_spike_latency('L2/3',max_I_timepoints[np.where(np.logical_and(max_I_timepoints>3000, max_I_timepoints<4000))])[0])+ ' ms')\n",
    "# fig3.axes[1].set_title('L2/3 first spike jitter: '+str(calc_first_spike_latency('L2/3',max_I_timepoints[np.where(np.logical_and(max_I_timepoints>3000, max_I_timepoints<4000))])[1])+ ' ms')\n",
    "# fig3.axes[-1].text(0.5,-0.1,simulation_label,\n",
    "#         horizontalalignment='left',\n",
    "#         verticalalignment='top',\n",
    "#         transform=fig3.axes[-1].transAxes)\n",
    "\n",
    "# if save_plots == True:\n",
    "#     fig3.savefig('simulations/'+simulation_label+'/'+epochs[2]+'_firstspike.pdf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o_W3JGB4S-I5"
   },
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pkej6afSS-I5"
   },
   "source": [
    "#### Plot neuronal spike times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Nhpo763S-I6",
    "outputId": "fe25554b-400d-4110-d8d8-5e410ffcdcb2",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot neuronal spike times\n",
    "fig, axes = plt.subplots(len(population_parameters), 1, sharex=True, figsize=(12, 10))\n",
    "for ax, label in zip(axes, population_parameters):\n",
    "    ax.plot(spike_monitor[label].t/b2.ms, spike_monitor[label].i, '|',\n",
    "            markersize = 3.,\n",
    "            color      = population_parameters[label]['color'],\n",
    "            alpha      = 1.,\n",
    "            rasterized = True)\n",
    "    ax.set_ylabel(population_parameters[label]['name'])\n",
    "    ax.set_xlim(balancing_time, total_time)\n",
    "    \n",
    "fig.axes[-1].text(0,-0.2,simulation_label,\n",
    "        horizontalalignment='left',\n",
    "        verticalalignment='top',\n",
    "        transform=ax.transAxes)\n",
    "\n",
    "fig.suptitle(general_simulation_title)\n",
    "    \n",
    "if save_plots == True:\n",
    "    fig.savefig('simulations/'+simulation_label+'/raster_all_populations.pdf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gv_NznQ_S-I8"
   },
   "source": [
    "#### Plot population firing rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "befyC53GS-I8",
    "outputId": "7de8f93b-f47b-4d03-90ee-2495470ef17d",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(len(population_parameters), 1, sharex=True, sharey=True, figsize=(12, 10))\n",
    "\n",
    "for ax, label in zip(axes,population_parameters):\n",
    "    ax.plot(rate_monitor[label].t/b2.ms,\n",
    "            rate_monitor[label].smooth_rate(window = 'gaussian', width = 25*b2.ms)/b2.Hz,\n",
    "            label = population_parameters[label]['name'],\n",
    "            color = population_parameters[label]['color'])\n",
    "    ax.set_ylabel('Firing rate [Hz]')\n",
    "    ax.legend(loc='best')\n",
    "    ax.set_xlim(balancing_time, total_time)\n",
    "    for interval_start in np.arange(balancing_time,total_time,interval_length*2):\n",
    "        ax.fill_between(np.arange(interval_start,interval_start+interval_length,1),0,60,alpha=0.05,facecolor='green')\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.axes[-1].text(0,-0.2,simulation_label,\n",
    "        horizontalalignment='left',\n",
    "        verticalalignment='top',\n",
    "        transform=ax.transAxes)\n",
    "fig.suptitle(general_simulation_title)\n",
    "\n",
    "if save_plots == True:\n",
    "    fig.savefig('simulations/'+simulation_label+'/FR_all_populations.pdf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mf9iXljoS-I-"
   },
   "source": [
    "#### Plot L2/3 single neuron Vm\n",
    "\n",
    "- 3 neurons (random sampling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2yr64gqvS-I-",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set_background('lightblue')\n",
    "\n",
    "# label = 'L2/3'\n",
    "# neuron_no = 3\n",
    "# fig, axes = plt.subplots(neuron_no+1, 1, sharex=True, figsize=(12, 10))\n",
    "\n",
    "# for counter,ii in zip(np.arange(1,(neuron_no+1)),np.random.choice(spike_monitor[label].i, size=neuron_no)):\n",
    "#     times, sender = zip(*[(t, i) for (t, i) in zip(spike_monitor[label].t/b2.ms, spike_monitor[label].i) if i == ii])\n",
    "#     axes[0].plot(times, sender, '|',\n",
    "#                  markersize = 5.,\n",
    "#                  color      = population_parameters[label]['color'],\n",
    "#                  alpha      = 1.,\n",
    "#                  rasterized = True)\n",
    "#     axes[0].set_ylabel('Neuron ID')\n",
    "    \n",
    "#     axes[counter].plot(state_monitor[label].t/b2.ms,\n",
    "#             state_monitor[label].variables['v'].get_value()[:,ii]*1000)\n",
    "#     axes[counter].set_xlim(balancing_time, total_time)\n",
    "#     axes[counter].fill_between(np.arange(1000,2000,1),axes[counter].get_ylim()[0],axes[counter].get_ylim()[1],alpha=0.05,facecolor='green')\n",
    "#     axes[counter].fill_between(np.arange(2000,1000,1),axes[counter].get_ylim()[0],axes[counter].get_ylim()[1],alpha=0.05,facecolor='cyan')\n",
    "#     axes[counter].fill_between(np.arange(2000,1000,1),axes[counter].get_ylim()[0],axes[counter].get_ylim()[1],alpha=0.05,facecolor='cyan')\n",
    "#     axes[counter].set_xlim(balancing_time, total_time)\n",
    "#     axes[counter].set_ylabel('Vm (mV)')\n",
    "\n",
    "# fig.axes[-1].set_xlabel('Time (ms)')\n",
    "# fig.suptitle('L2/3 Vm')\n",
    "# fig.align_ylabels()\n",
    "# fig.tight_layout()\n",
    "# fig.axes[-1].text(0,-0.1,simulation_label,\n",
    "#         horizontalalignment='left',\n",
    "#         verticalalignment='top',\n",
    "#         transform=ax.transAxes)\n",
    "\n",
    "\n",
    "# if save_plots == True:\n",
    "#     fig.savefig('simulations/'+simulation_label+'/single_L23_Vm.pdf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dJEZEpgaS-JA",
    "outputId": "95f96ac6-255e-4665-d2ce-8a741fff721e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.signal import find_peaks,peak_widths\n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "# amplitudes = dict()\n",
    "# hwhm = dict()\n",
    "# peak_times = dict()\n",
    "\n",
    "\n",
    "peaks_df = pd.DataFrame(columns=['epoch','population','amplitude','hwhm','peak_time'])\n",
    "\n",
    "for label in population_parameters:\n",
    "    \n",
    "    FR_temp = rate_monitor[label].smooth_rate(window = 'gaussian', width = 5*b2.ms)/b2.Hz\n",
    "\n",
    "    for ii,epoch in enumerate(np.arange(balancing_time,total_time,interval_length)):\n",
    "        epoch_window = np.arange(epoch*10,epoch*10+interval_length*10)\n",
    "        epoch_rate_temp = FR_temp[epoch_window]\n",
    "        \n",
    "        if label == 'L2/3':\n",
    "            peaks_temp = find_peaks(epoch_rate_temp,distance = 20*10, height = 0)\n",
    "\n",
    "            peak_timepoints_indices = []\n",
    "            for cycle_start in np.arange(0,interval_length*10,duty_cycle_length*10):\n",
    "                for index,peak in enumerate(peaks_temp[0]):\n",
    "                    if cycle_start< peak < cycle_start+40*10:\n",
    "                        peak_timepoints_indices.append(index)\n",
    "\n",
    "\n",
    "            peak_time_temp = peaks_temp[0][peak_timepoints_indices]\n",
    "\n",
    "            hwhm_temp = peak_widths(epoch_rate_temp,peak_time_temp)[0]\n",
    "            hwhm_temp = hwhm_temp/10\n",
    "\n",
    "            peak_time_temp = peak_time_temp +epoch*10\n",
    "            peak_time_temp = peak_time_temp/10\n",
    "\n",
    "\n",
    "            amplitudes_temp = peaks_temp[1]['peak_heights'][peak_timepoints_indices]\n",
    "            \n",
    "        else:\n",
    "            peak_time_temp = find_peaks(epoch_rate_temp,distance = duty_cycle_length/5*10, height = 0)[0]\n",
    "\n",
    "            hwhm_temp = peak_widths(epoch_rate_temp,peak_time_temp)[0]\n",
    "            hwhm_temp = hwhm_temp/10\n",
    "\n",
    "            peak_time_temp = peak_time_temp +epoch*10\n",
    "\n",
    "            peak_time_temp = peak_time_temp/10\n",
    "\n",
    "            amplitudes_temp = find_peaks(epoch_rate_temp,distance = duty_cycle_length/5*10, height = 0)[1]['peak_heights']\n",
    "    #         if len(amplitudes_temp)>1:\n",
    "    #             amplitudes_temp = amplitudes_temp[0]\n",
    "\n",
    "    #         amplitudes_temp = amplitudes_temp[np.where(peak_time_temp>balancing_time*10)]\n",
    "\n",
    "    #         amplitudes[label] = amplitudes_temp\n",
    "\n",
    "    #         hwhm_temp = peak_widths(epoch_rate_temp,peak_time_temp[np.where(peak_time_temp>balancing_time*10)],0.5)[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        new_entry = {'epoch':epochs[ii],\n",
    "                     'population':label,\n",
    "                     'amplitude':amplitudes_temp[1:],\n",
    "                     'hwhm':hwhm_temp[1:],\n",
    "                     'peak_time':peak_time_temp[1:]\n",
    "                     }\n",
    "\n",
    "\n",
    "        peaks_df = peaks_df.append(new_entry,ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "peaks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d8n1jui7S-JC",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "# return list from series of comma-separated strings\n",
    "def chainer(s):\n",
    "    return list(chain.from_iterable(s))\n",
    "\n",
    "# calculate lengths of splits\n",
    "lens = peaks_df['peak_time'].map(len)\n",
    "# create new dataframe, repeating or chaining as appropriate\n",
    "res = pd.DataFrame({'epoch': np.repeat(peaks_df['epoch'], lens),\n",
    "                    'population': np.repeat(peaks_df['population'], lens),\n",
    "                    'peak_time': chainer(peaks_df['peak_time']),\n",
    "                    'amplitude': chainer(peaks_df['amplitude']),\n",
    "                    'hwhm': chainer(peaks_df['hwhm'])\n",
    "                   })\n",
    "if save_plots==True:\n",
    "    res.to_csv('simulations/'+simulation_label+'/population_peaks.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IxCalmUBS-JG",
    "outputId": "6d97a612-f3c6-4442-87ed-8801b19dbbb5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "res.loc[res['population']=='L2/3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JQyQUAYQS-JI",
    "outputId": "05d55b40-0033-4df1-a484-b071a23d74b6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "colors = sns.color_palette(\"tab20\", len(epochs))\n",
    "\n",
    "populations = ['L2/3', 'L4', 'L5a', 'L5b', 'PV']\n",
    "populations_multiindex = np.repeat(populations,len(epochs))\n",
    "\n",
    "epochs_multiindex = np.tile(epochs,5)\n",
    "\n",
    "multiindex_plot = pd.MultiIndex.from_arrays([populations_multiindex,epochs_multiindex],names=('population', 'epoch') )\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1,figsize=(20,20))\n",
    "\n",
    "\n",
    "for i,measure in enumerate(['amplitude','hwhm']):\n",
    "\n",
    "    mean_s = res.set_index('epoch').groupby(['population', 'epoch'])[measure].mean()\n",
    "    mean_s = mean_s.reindex(multiindex_plot)\n",
    "    sem_s = res.set_index('epoch').groupby(['population', 'epoch'])[measure].sem()\n",
    "    sem_s = sem_s.reindex(multiindex_plot)\n",
    "\n",
    "    mean_df = mean_s.to_frame()\n",
    "    mean_df['sem'] = sem_s\n",
    "    mean_df_plot = mean_df.reset_index()\n",
    "    pivoted_mean_df = mean_df_plot.pivot(index='population',columns='epoch',values=measure)\n",
    "    pivoted_mean_df = pivoted_mean_df.reindex(columns = epochs, fill_value =0)\n",
    "    pivoted_mean_df.plot(ax = axes[i],\n",
    "                         kind='bar',\n",
    "                         color = colors,\n",
    "                         yerr=mean_df_plot.pivot(index='epoch',columns='population',values='sem').values\n",
    "                         )\n",
    "    axes[i].set_title('Population response : ' + measure,fontsize = 30)\n",
    "    axes[i].xaxis.label.set_size(25)\n",
    "    axes[i].yaxis.label.set_size(25)\n",
    "    axes[i].legend(fontsize=10, loc='upper center',ncol=2)\n",
    "    axes[i].tick_params(axis='both', which='major', labelsize=15)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "axes[0].set_ylabel('spk/s', fontsize=20)\n",
    "axes[1].set_ylabel('ms', fontsize=20)\n",
    "axes[0].titlesize=30\n",
    "axes[0].titlesize=30\n",
    "axes[1].labelsize=50\n",
    "axes[1].labelsize=50\n",
    "\n",
    "fig.axes[-1].text(0,0.5,simulation_label,\n",
    "        horizontalalignment='left',\n",
    "        verticalalignment='top',\n",
    "        transform=ax.transAxes)\n",
    "\n",
    "fig.suptitle(general_simulation_title,fontsize=35)\n",
    "\n",
    "    \n",
    "\n",
    "plt.subplots_adjust(hspace=0.35)\n",
    "\n",
    "if save_plots == True:\n",
    "    fig.savefig('simulations/'+simulation_label+'/FR_peakamp_hwhm_all.pdf')\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# custom_cycler = cycler(color=colors)\n",
    "\n",
    "# ax1.set_prop_cycle(custom_cycler)\n",
    "# ax1.plot(yy)\n",
    "######\n",
    "test_L4 = rate_monitor['L4']\n",
    "epoch_rate_per_cycle_mean_L4 = np.zeros((duty_cycle_length*10,len(epochs)))\n",
    "epoch_rate_per_cycle_sem_L4 = np.zeros((duty_cycle_length*10,len(epochs)))\n",
    "\n",
    "\n",
    "for ii,epoch in enumerate(np.arange(balancing_time,total_time,interval_length)):\n",
    "    epoch_window = np.arange(epoch*10,epoch*10+interval_length*10)\n",
    "    rate_epoch_temp = (test_L4.smooth_rate(window = 'gaussian', width = 5*b2.ms)/b2.Hz)[epoch_window]\n",
    "\n",
    "    epoch_rate_per_cycle = np.zeros((duty_cycle_length*10,int(interval_length/duty_cycle_length)))\n",
    "    for i,cycle in enumerate(np.arange(epoch_window[0],epoch_window[-1]-duty_cycle_length*10,duty_cycle_length*10)):\n",
    "        cycle_window = np.arange(cycle,cycle+duty_cycle_length*10)\n",
    "        rate_cycle_temp = (test_L4.smooth_rate(window = 'gaussian', width = 5*b2.ms)/b2.Hz)[cycle_window]\n",
    "        epoch_rate_per_cycle[:,i] = rate_cycle_temp\n",
    "    \n",
    "    sem_temp = sem(epoch_rate_per_cycle,1)\n",
    "    epoch_rate_per_cycle_mean_temp = np.mean(epoch_rate_per_cycle,1)\n",
    "    \n",
    "    epoch_rate_per_cycle_mean_L4[:,ii] = epoch_rate_per_cycle_mean_temp\n",
    "    epoch_rate_per_cycle_sem_L4[:,ii] = sem_temp\n",
    "######\n",
    "\n",
    "test_r = rate_monitor['L2/3']\n",
    "epoch_rate_per_cycle_mean = np.zeros((duty_cycle_length*10,len(epochs)))\n",
    "epoch_rate_per_cycle_sem = np.zeros((duty_cycle_length*10,len(epochs)))\n",
    "\n",
    "\n",
    "for ii,epoch in enumerate(np.arange(balancing_time,total_time,interval_length)):\n",
    "    epoch_window = np.arange(epoch*10,epoch*10+interval_length*10)\n",
    "#     print(epoch_window)\n",
    "    rate_epoch_temp = (test_r.smooth_rate(window = 'gaussian', width = 5*b2.ms)/b2.Hz)[epoch_window]\n",
    "\n",
    "    epoch_rate_per_cycle = np.zeros((duty_cycle_length*10,int(interval_length/duty_cycle_length)))\n",
    "    for i,cycle in enumerate(np.arange(epoch_window[0],epoch_window[-1]-duty_cycle_length*10,duty_cycle_length*10)):\n",
    "        cycle_window = np.arange(cycle,cycle+duty_cycle_length*10)\n",
    "        rate_cycle_temp = (test_r.smooth_rate(window = 'gaussian', width = 5*b2.ms)/b2.Hz)[cycle_window]\n",
    "        epoch_rate_per_cycle[:,i] = rate_cycle_temp\n",
    "    \n",
    "    sem_temp = sem(epoch_rate_per_cycle,1)\n",
    "    epoch_rate_per_cycle_mean_temp = np.mean(epoch_rate_per_cycle,1)\n",
    "    \n",
    "    epoch_rate_per_cycle_mean[:,ii] = epoch_rate_per_cycle_mean_temp\n",
    "    epoch_rate_per_cycle_sem[:,ii] = sem_temp\n",
    "\n",
    "    \n",
    "no_rows,remaining = divmod(len(epochs),4)\n",
    "fig, axes = plt.subplots(no_rows +1 if remaining else no_rows, 4, sharex=True, sharey=True, figsize=(16, 10))\n",
    "\n",
    "axes = np.array(axes)\n",
    "\n",
    "for counter,ax in enumerate(axes.reshape(-1)):\n",
    "\n",
    "\n",
    "#     ax = plt.axes()\n",
    "#     ax.set_prop_cycle(custom_cycler)\n",
    "    if counter <= len(epochs)-1:\n",
    "        line1, = ax.plot(epoch_rate_per_cycle_mean[:,counter], color=colors[counter])\n",
    "        line2, = ax.plot(epoch_rate_per_cycle_mean_L4[:,counter], color = 'k', alpha = 0.8)\n",
    "        ax.legend((line1, line2), ('L2/3 ('+epochs[counter]+')', 'L4'))\n",
    "        ax.set_xlim(0,2500)\n",
    "        ax.set_ylim(0,int(res[res.population.eq('L2/3')].amplitude.max())+1)\n",
    "        ax.set_xticks(ax.get_xticks().tolist()) # REMOVE IN THE FUTURE - PLACED TO AVOID WARNING - IT IS A BUG FROM MATPLOTLIB 3.3.1\n",
    "        ax.set_xticklabels(np.arange(0,300,50).astype(str))\n",
    "        if counter in [0,4,8]:\n",
    "            ax.set_ylabel('spks/s')\n",
    "        if counter in [8,9,10,11]:\n",
    "            ax.set_xlabel('ms')\n",
    "\n",
    "        ax.fill_between(np.arange(0,duty_cycle_length*10),\n",
    "                        epoch_rate_per_cycle_mean[:,counter]-epoch_rate_per_cycle_sem[:,counter], \n",
    "                        epoch_rate_per_cycle_mean[:,counter]+epoch_rate_per_cycle_sem[:,counter],\n",
    "                        alpha=0.3, color=colors[counter],linewidth=0)\n",
    "        ax.fill_between(np.arange(0,duty_cycle_length*10),\n",
    "                        epoch_rate_per_cycle_mean_L4[:,counter]-epoch_rate_per_cycle_sem_L4[:,counter], \n",
    "                        epoch_rate_per_cycle_mean_L4[:,counter]+epoch_rate_per_cycle_sem_L4[:,counter],\n",
    "                        alpha=0.3, color= 'k',linewidth=0)\n",
    "        \n",
    "        \n",
    "\n",
    "fig.suptitle(general_simulation_title, fontsize=16)\n",
    "\n",
    "fig.tight_layout()\n",
    "    \n",
    "if save_plots==True:\n",
    "    fig.savefig('simulations/'+simulation_label+'/L23_mean_response_curves.pdf')\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "model_v1_discreteL5aL5b-temporaloffset.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
